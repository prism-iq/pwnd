# MÃ©thodologie de Debugging RAG - Documentation

Cette documentation capture toute la mÃ©thodologie apprise lors du debugging du systÃ¨me L Investigation Framework.

## ðŸ“ Fichiers Disponibles

### 1. `METHODOLOGY_PROMPT.md` (19K, 633 lignes) â­ VERSION COMPLÃˆTE
**Utilisation:** Formation approfondie, rÃ©fÃ©rence complÃ¨te

**Contenu:**
- Architecture dÃ©taillÃ©e du systÃ¨me RAG (4 Ã©tapes)
- MÃ©thodologie de diagnostic bottom-up complÃ¨te
- Exemples concrets de chaque problÃ¨me rencontrÃ©
- Solutions avec code Python complet
- Explication des choix techniques (pourquoi Haiku, pourquoi bypass Phi-3)
- Tests d'isolation pour chaque couche
- Logging stratÃ©gique avec exemples
- Prompt engineering (gros vs petits modÃ¨les)
- Anti-patterns dÃ©taillÃ©s
- Workflow de debugging complet
- Version synthÃ©tique incluse Ã  la fin

**Quand l'utiliser:**
- Formation d'un nouveau dÃ©veloppeur sur le systÃ¨me
- Comprendre EN PROFONDEUR la mÃ©thodologie
- RÃ©fÃ©rence lors de problÃ¨mes complexes
- Documentation pour l'Ã©quipe

---

### 2. `DEBUG_CHEATSHEET.md` (2.5K, 100 lignes) âš¡ VERSION RAPIDE
**Utilisation:** RÃ©fÃ©rence rapide pendant le debugging

**Contenu:**
- Checklist de diagnostic (5 Ã©tapes)
- Golden rules (Test Isolation, LLM Strategy, Logging, CSS/HTML)
- Quick fixes prÃªts Ã  copier/coller
- Tableaux de rÃ©fÃ©rence (quels modÃ¨les, quelle approche)
- Anti-patterns (ce qu'il NE faut PAS faire)
- Pipeline RAG avec latences et coÃ»ts
- Checklist "Stuck?"

**Quand l'utiliser:**
- En plein debugging, besoin d'une rÃ©fÃ©rence rapide
- Rappel des golden rules
- Copier/coller des quick fixes
- Checklist quand on est bloquÃ©

**Format:** 1 page A4, imprimable, facile Ã  scanner

---

### 3. `PROMPT_READY_TO_USE.txt` (3.7K, 106 lignes) ðŸ¤– COPIER/COLLER
**Utilisation:** Prompt systÃ¨me pour un LLM

**Contenu:**
- MÃ©thodologie de diagnostic condensÃ©e
- RÃ¨gles d'or
- Quick fixes
- Anti-patterns
- Pipeline RAG
- LLM strategy
- Workflow debugging
- Checklist

**Quand l'utiliser:**
- Copier/coller ce texte dans un chat LLM (Claude, GPT-4)
- System prompt pour un agent de debugging
- Partager rapidement la mÃ©thodologie (email, Slack)
- Onboarding rapide d'un collaborateur

**Format:** Plain text, optimisÃ© pour Ãªtre utilisÃ© comme prompt

---

## ðŸŽ¯ Cas d'Usage

### ScÃ©nario 1: Nouveau Bug RAG
1. Ouvrir `DEBUG_CHEATSHEET.md` â†’ Checklist de diagnostic
2. Suivre les 5 Ã©tapes (Data â†’ SQL â†’ LLM â†’ API â†’ UI)
3. Si bloquÃ© â†’ "Stuck? Checklist"
4. Si besoin de comprendre pourquoi â†’ `METHODOLOGY_PROMPT.md`

### ScÃ©nario 2: Former un DÃ©veloppeur
1. Lire `METHODOLOGY_PROMPT.md` (version dÃ©taillÃ©e)
2. Pratiquer avec des exemples rÃ©els
3. Garder `DEBUG_CHEATSHEET.md` comme rÃ©fÃ©rence quotidienne

### ScÃ©nario 3: Utiliser un LLM pour Debugger
1. Copier le contenu de `PROMPT_READY_TO_USE.txt`
2. Coller dans Claude/GPT-4 comme system prompt
3. DÃ©crire le bug
4. Le LLM appliquera la mÃ©thodologie

### ScÃ©nario 4: Code Review
1. VÃ©rifier les anti-patterns dans `DEBUG_CHEATSHEET.md`
2. S'assurer que le code suit les golden rules
3. VÃ©rifier la stratÃ©gie LLM (gros vs petit modÃ¨le)

---

## ðŸ“Š MÃ©thodologie en Bref

### Diagnostic Bottom-Up (Toujours dans cet ordre!)
```
1. Data    â†’ sqlite3 "SELECT COUNT(*)"
2. SQL     â†’ Test query directe
3. LLM     â†’ curl endpoint direct
4. API     â†’ journalctl + curl /health
5. UI      â†’ Inspect HTML vs CSS
```

### Golden Rules
1. **Test Isolation:** UN composant Ã  la fois, jamais le systÃ¨me complet d'abord
2. **Read, Don't Assume:** Lire le code/HTML rÃ©el, jamais assumer
3. **Small Models = Code:** Phi-3/Mistral-7B â†’ Code Python > Prompts complexes
4. **Strategic Logs:** Valeurs concrÃ¨tes (`f"q={q} â†’ {len(r)} results"`)
5. **Restart Properly:** `systemctl restart && sleep 3 && status`

### Quick Fixes Essentiels

**LLM Output Parsing:**
```python
for line in response.split('\n'):
    if line.startswith('{'):
        try:
            return json.loads(line)
        except:
            continue
```

**Bypass LLM Instable:**
```python
response = " ".join(findings)
response = response.replace("bad phrase", "good phrase")
response += f"\n\nSources: {sources}"
```

**CSS/HTML Mismatch:**
```bash
grep 'class=\|id=' index.html  # Lire AVANT d'Ã©crire CSS
```

---

## ðŸš€ Exemples Concrets (Bugs RÃ©solus)

### Bug 1: "No Results" pour "Who is Jeffrey Epstein?"
- **SymptÃ´me:** API retourne "I couldn't find relevant documents"
- **Diagnostic:** Data OK (13009 emails) â†’ SQL OK (1130 matches) â†’ LLM parsing FAIL
- **Root cause:** Phi-3 retournait `"- response: {...}"` â†’ JSON parser Ã©chouait â†’ fallback `entities=[]`
- **Fix:** Multiline JSON parsing avec extraction du premier `{...}` valide
- **Fichier:** `METHODOLOGY_PROMPT.md` Section 2.2

### Bug 2: "Discord Look Like Tetris"
- **SymptÃ´me:** CSS Discord appliquÃ© mais layout cassÃ©
- **Diagnostic:** CSS OK â†’ HTML lu â†’ MISMATCH trouvÃ©
- **Root cause:** CSS `.chat-container` mais HTML `.messages-container`
- **Fix:** Mapper exactement CSS classes sur HTML classes
- **Fichier:** `METHODOLOGY_PROMPT.md` Section 3

### Bug 3: Phi-3 GÃ©nÃ¨re du HTML au lieu de Texte
- **SymptÃ´me:** Response = `"<div style='text-align:justify'>..."`
- **Diagnostic:** Prompt trop complexe pour Phi-3-Mini
- **Root cause:** 50 lignes de rÃ¨gles â†’ Phi-3 confused â†’ hallucination
- **Fix:** Bypass Phi-3, faire du string templating Python
- **Fichier:** `METHODOLOGY_PROMPT.md` Section 5

---

## ðŸ’¡ Principes ClÃ©s Appris

1. **Bottom-Up Debugging:** Toujours commencer par la couche la plus basse (data)
2. **Isolation Testing:** Tester chaque composant sÃ©parÃ©ment avant le systÃ¨me
3. **Model Awareness:** Phi-3 â‰  Mistral â‰  Claude, adapter la stratÃ©gie
4. **Cost Optimization:** Local LLM pour parsing/formatage, API pour analyse
5. **Simplicity Wins:** Code Python simple > Prompts complexes pour petits modÃ¨les
6. **Monitor Patterns:** Logs de warning pour dÃ©tecter les phrasings problÃ©matiques
7. **Verify Everything:** Ne jamais assumer (CSS matche, model = nom fonction)

---

## ðŸ“ˆ MÃ©triques SystÃ¨me

**Pipeline OptimisÃ©:**
```
Step 1: Intent Parse (Phi-3, 2s, $0)
Step 2: SQL Execute (Python, 0.1s, $0)
Step 3: Analyze (Haiku, 5s, $0.0004)
Step 4: Format (Python, 0ms, $0)

Total: 5-8s latency, $0.0004/query
```

**Cost per 1000 queries:** $0.40

---

## ðŸ”— Liens Rapides

- **DÃ©tails complets:** `/opt/rag/METHODOLOGY_PROMPT.md`
- **Cheatsheet rapide:** `/opt/rag/DEBUG_CHEATSHEET.md`
- **Prompt LLM:** `/opt/rag/PROMPT_READY_TO_USE.txt`
- **Ce README:** `/opt/rag/METHODOLOGY_README.md`

---

**DerniÃ¨re mise Ã  jour:** 2026-01-08
**BasÃ© sur:** Session de debugging L Investigation Framework
**SystÃ¨me:** RAG 4-step pipeline (Phi-3 + SQLite FTS5 + Claude Haiku)
