# RAG Debugging - Cheatsheet Ultra-SynthÃ©tique

## Ordre de Diagnostic
```
1. Data    â†’ sqlite3 "SELECT COUNT(*)"
2. SQL     â†’ Test query directe
3. LLM     â†’ curl endpoint direct
4. API     â†’ journalctl -u service
5. UI      â†’ Inspect element HTML vs CSS
```

## Golden Rules

### ğŸ” Test Isolation
```bash
âŒ curl /api/ask?q=test                    # SystÃ¨me complet
âœ“  sqlite3 db "SELECT * FROM ..."         # Data layer
âœ“  curl localhost:8001/generate           # LLM layer
âœ“  curl localhost:8002/api/health         # API layer
```

### ğŸ¤– LLM Strategy
| Model | Approach |
|-------|----------|
| Claude Opus, GPT-4 | Prompts complexes OK |
| Phi-3, Mistral-7B, Llama-7B | **Code Python > Prompts** |

### ğŸ“ Logging
```python
âŒ logging.info("Processing...")
âœ“  logging.info(f"q='{query}' â†’ {len(results)} results")
```

### ğŸ¨ CSS/HTML
```bash
grep 'class=\|id=' index.html  # Lire AVANT d'Ã©crire CSS
```

### ğŸ”„ Service Restart
```bash
systemctl restart srv && sleep 3 && systemctl status srv
#                        ^^^^^^^^ CRITIQUE
```

## Quick Fixes

### Intent Parsing (LLM retourne du bruit)
```python
for line in response.split('\n'):
    if line.startswith('-'):
        line = line.split(':', 1)[-1].strip()
    if line.startswith('{'):
        try:
            return json.loads(line)
        except:
            continue
```

### Response Formatting (Bypass LLM instable)
```python
# Au lieu de prompt complexe â†’ LLM
findings = haiku_json.get("findings", [])
response = " ".join(findings)
response = response.replace("LinkedIn profile", "LinkedIn emails")
response += f"\n\nSources: {sources}"
```

### CSS Mismatch
```python
# HTML: <div class="messages-container">
# CSS:  .messages-container { }  â† Match EXACT
```

## Anti-Patterns
- âŒ Assumer (CSS matche, model = nom fonction)
- âŒ Prompts 50 lignes pour Phi-3
- âŒ Tester systÃ¨me complet d'abord
- âŒ Oublier `sleep` aprÃ¨s restart

## Pipeline RAG Typique
```
Intent (Phi-3, 2s, $0) â†’ SQL (0.1s, $0) â†’ Analyze (Haiku, 5s, $0.0004) â†’ Format (Python, 0ms, $0)
```

## Stuck? Checklist
1. [ ] Read actual code (don't assume)
2. [ ] Test ONE component isolated
3. [ ] Add logs with VALUES
4. [ ] Check ACTUAL model running
5. [ ] Simplify (code > prompts)
6. [ ] Restart with `sleep 3`

## Cost Optimization
- Local LLM: Intent parsing + formatage â†’ **$0**
- API LLM: Analyse ONLY â†’ **$0.0004/query**
- Total: **~$0.0004/query** (200 queries = $0.08)

---

**MÃ©thodologie complÃ¨te:** `/opt/rag/METHODOLOGY_PROMPT.md`
